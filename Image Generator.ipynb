{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/lusy/Desktop/MasterThesis\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "import glob, os\n",
    "\n",
    "#os.chdir(\"..\")\n",
    "print(os.getcwd())\n",
    "\n",
    "#os.listdir(os.getcwd())\n",
    "#os.chdir(\"My Drive/Colab Notebooks/images\")\n",
    "#print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        #rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        #zoom_range=0.2,\n",
    "        #horizontal_flip=True,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "\n",
    "for file in glob.glob(\"weird images/NeckerCubeDrawings/healthy/\" + \"*.png\"):\n",
    "    img = load_img(file)  # this is a PIL image\n",
    "    x = img_to_array(img)  # this is a Numpy array with shape (3, 150, 150)\n",
    "    x = x.reshape((1,) + x.shape)  # this is a Numpy array with shape (1, 3, 150, 150)\n",
    "\n",
    "# the .flow() command below generates batches of randomly transformed images\n",
    "# and saves the results to the `preview/` directory\n",
    "    i = 0\n",
    "    for batch in datagen.flow(x, batch_size=1,\n",
    "                          save_to_dir='weird images/NeckerCubeDrawings/healthy', save_format='png'):\n",
    "        i += 1\n",
    "        if i > 15:\n",
    "            break \n",
    "            \n",
    "for file in glob.glob(\"weird images/NeckerCubeDrawings/PD/\" + \"*.png\"):\n",
    "    img = load_img(file)  # this is a PIL image\n",
    "    x = img_to_array(img)  # this is a Numpy array with shape (3, 150, 150)\n",
    "    x = x.reshape((1,) + x.shape)  # this is a Numpy array with shape (1, 3, 150, 150)\n",
    "\n",
    "# the .flow() command below generates batches of randomly transformed images\n",
    "# and saves the results to the `preview/` directory\n",
    "    i = 0\n",
    "    for batch in datagen.flow(x, batch_size=1,\n",
    "                          save_to_dir='weird images/NeckerCubeDrawings/healthy', save_format='png'):\n",
    "        i += 1\n",
    "        if i > 10:\n",
    "            break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2993, 2993)"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "\n",
    "file_list = []\n",
    "class_list = []\n",
    "\n",
    "DATADIR = \"weird images/NeckerCubeDrawings\"\n",
    "\n",
    "# All the categories you want your neural network to detect\n",
    "CATEGORIES = [\"healthy\", \"PD\"]\n",
    "\n",
    "# The size of the images that your neural network will use\n",
    "IMG_SIZE = 150\n",
    "\n",
    "# Checking or all images in the data folder\n",
    "for category in CATEGORIES :\n",
    "\tpath = os.path.join(DATADIR, category)\n",
    "\tfor img in os.listdir(path):\n",
    "\t\timg_array = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "training_data = []\n",
    "\n",
    "def create_training_data():\n",
    "\tfor category in CATEGORIES :\n",
    "\t\tpath = os.path.join(DATADIR, category)\n",
    "\t\tclass_num = CATEGORIES.index(category)\n",
    "\t\tfor img in os.listdir(path):\n",
    "\t\t\ttry :\n",
    "\t\t\t\timg_array = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n",
    "\t\t\t\tnew_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\n",
    "\t\t\t\ttraining_data.append([new_array, class_num])\n",
    "\t\t\texcept Exception as e:\n",
    "\t\t\t\tpass\n",
    "\n",
    "create_training_data() # contains training_data with images and classes\n",
    "\n",
    "random.shuffle(training_data) #shufles the data so it's not control and patient\n",
    "\n",
    "X = [] #features\n",
    "y = [] #labels\n",
    "#y = np.array(y)\n",
    "\n",
    "for features, label in training_data:\n",
    "\tX.append(features)\n",
    "\ty.append(label)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\"\"\"\"\"\"\n",
    "# Creating the files containing all the information about your model\n",
    "pickle_out = open(\"X.pickle\", \"wb\")\n",
    "pickle.dump(X, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(\"y.pickle\", \"wb\")\n",
    "pickle.dump(y, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_in = open(\"X.pickle\", \"rb\")\n",
    "X = pickle.load(pickle_in)\n",
    "\n",
    "\n",
    "#y=np.array(y)\n",
    "#y.shape\n",
    "\n",
    "\"\"\"\n",
    "len(X),len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_datatest(x,y):\n",
    "\n",
    "    #Â train and test\n",
    "    X_train = X[:int(0.9*int(len(X)))]\n",
    "    y_train = y[:int(0.9*int(len(X)))]\n",
    "    X_test = X[int(0.9*int(len(X))):]\n",
    "    y_test = y[int(0.9*int(len(y))):]\n",
    "    \n",
    "    # validation\n",
    "    X_train = X_train[:int(0.8*int(len(X_train)))]\n",
    "    y_train = y_train[:int(0.8*int(len(y_train)))]\n",
    "    \n",
    "    X_val = X_train[int(0.8*int(len(X_train))):]\n",
    "    y_val = y_train[int(0.8*int(len(y_train))):]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test, X_val, y_val\n",
    "\n",
    "#len(X_val), len(y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_validation(X_train,y_train):\n",
    "    X_train = X_train[:int(0.8*int(len(X_train)))]\n",
    "    y_train = y_train[:int(0.8*int(len(y_train)))]\n",
    "    \n",
    "    X_val = X_train[int(0.8*int(len(X_train))):]\n",
    "    y_val = y_train[int(0.8*int(len(y_train))):]\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "431 431 2154 2154 300 300\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train, y_train, X_test, y_test, X_val, y_val = split_datatest(X,y)\n",
    "\n",
    "print(len(X_val), len(y_val),len(X_train), len(y_train), len(X_test), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(X_train, y_train, X_test, y_test, X_val, y_val):\n",
    "\n",
    "    # reshape training sets to prepare for CNN\n",
    "    X_train = np.array(X_train).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "    y_train = np.array(y_train)\n",
    "    \n",
    "    X_test = np.array(X_test).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "    y_test = np.array(y_test)\n",
    "    \n",
    "    X_val = np.array(X_val).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "    y_val = np.array(y_val)\n",
    "    \n",
    "    # one hot encoding of pixels\n",
    "    y_train = to_categorical(y_train)\n",
    "    y_test = to_categorical(y_test)\n",
    "    y_val = to_categorical(y_val)\n",
    "    \n",
    "    train_norm = X_train.astype('float32')\n",
    "    test_norm = X_test.astype('float32')\n",
    "    val_norm = X_val.astype('float32')\n",
    "\n",
    "    # normalize to range 0-1\n",
    "    X_train = train_norm / 255.0\n",
    "    X_test = test_norm / 255.0\n",
    "    X_val = val_norm / 255.0\n",
    "    # return dataset\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test, X_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test harness for evaluating models on the cifar10 dataset\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.datasets import cifar10, mnist\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.optimizers import SGD\n",
    "\n",
    " \n",
    "# define cnn model\n",
    "def define_model():\n",
    "  model = Sequential()\n",
    "  model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(150, 150, 1)))\n",
    "  model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "  model.add(MaxPooling2D((2, 2)))\n",
    "  model.add(Dropout(0.2))\n",
    "  model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "  model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "  model.add(MaxPooling2D((2, 2)))\n",
    "  model.add(Dropout(0.3))\n",
    "  model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "  model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "  model.add(MaxPooling2D((2, 2)))\n",
    "  model.add(Dropout(0.4))\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "  model.add(Dropout(0.5))\n",
    "  model.add(Dense(2, activation='softmax'))\n",
    "  # compile model\n",
    "  opt = SGD(lr=0.001, momentum=0.9)\n",
    "  model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "  print(\"Model defined\")\n",
    "  return model\n",
    " \n",
    "# plot diagnostic learning curves\n",
    "def summarize_diagnostics(history):\n",
    "\t# plot loss\n",
    "\tpyplot.subplot(211)\n",
    "\tpyplot.title('Cross Entropy Loss')\n",
    "\tpyplot.plot(history.history['loss'], color='blue', label='train')\n",
    "\tpyplot.plot(history.history['val_loss'], color='orange', label='test')\n",
    "\t# plot accuracy\n",
    "\tpyplot.subplot(212)\n",
    "\tpyplot.title('Classification Accuracy')\n",
    "\tpyplot.plot(history.history['accuracy'], color='blue', label='train')\n",
    "\tpyplot.plot(history.history['val_accuracy'], color='orange', label='test')\n",
    "    #pyplot.show()\n",
    "\t# save plot to file\n",
    "\tfilename = sys.argv[0].split('/')[-1]\n",
    "\tpyplot.savefig(filename + '_plot.png')\n",
    "\tpyplot.close()\n",
    "    \n",
    " \n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness(x,y):\n",
    "    \n",
    "    #X_train, y_train, X_test, y_test = create_test(X,y)\n",
    "    \n",
    "    #X_train, y_train, X_val, y_val = create_validation(X_train,y_train)\n",
    "    \n",
    "    X_train, y_train, X_test, y_test, X_val, y_val = split_dataset(X,y)\n",
    "    \n",
    "    X_train, y_train, X_test, y_test, X_val, y_val = prepare_dataset(X_train, y_train, X_test, y_test, X_val, y_val)\n",
    "    \n",
    "    print(len(X_val), len(y_val))\n",
    "          \n",
    "    model = define_model()\n",
    "    # fit model\n",
    "    history = model.fit(X_train, y_train, epochs=10, batch_size=24, validation_data=(X_val, y_val), verbose=0)\n",
    "    # evaluate model\n",
    "    _, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print('Test Accuracy: %.3f' % (acc * 100.0))\n",
    "    # learning curves\n",
    "    summarize_diagnostics(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "431 431\n",
      "Model defined\n"
     ]
    }
   ],
   "source": [
    "run_test_harness(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
