{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copia de ImageGeneration.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOZFfWjWemZ4ofWnsqsBBX2",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parwisenlared/MasterThesis/blob/master/ImageGeneration_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ozNrHqvp_dFY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "d14a20b5-d6f7-4fae-9fb0-8c63dc5816b8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FuiQx2avBBDk",
        "colab": {}
      },
      "source": [
        "!unzip -uq \"/content/drive/My Drive/Colab Notebooks/dataset\" -d \"/content/drive/My Drive/Colab Notebooks/dataset\""
      ],
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zMW4PsbMBaeB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f3f80824-0a14-4aa6-b342-f99286a47343"
      },
      "source": [
        "import glob, os\n",
        "#os.chdir(\"..\")\n",
        "print(os.getcwd())\n",
        "\n",
        "#os.listdir(os.getcwd())\n",
        "#os.chdir(\"drive/My Drive/Colab Notebooks/\")\n",
        "#print(os.getcwd())"
      ],
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RrZmn8AUAveO",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "        #rotation_range=40,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        #shear_range=0.2,\n",
        "        #zoom_range=0.2,\n",
        "        #horizontal_flip=True,\n",
        "        fill_mode='nearest')\n",
        "\n",
        "\n",
        "for file in glob.glob(\"dataset/images/b&wZeroPressure/NeckerCubeDrawings/healthy/\" + \"*.png\"):\n",
        "    img = load_img(file)  # this is a PIL image\n",
        "    x = img_to_array(img)  # this is a Numpy array with shape (1, 200, 200)\n",
        "    x = x.reshape((1,) + x.shape)  # this is a Numpy array with shape (1, 200, 200)\n",
        "\n",
        "# the .flow() command below generates batches of randomly transformed images\n",
        "# and saves the results to the `preview/` directory\n",
        "    i = 0\n",
        "    for batch in datagen.flow(x, batch_size=1,\n",
        "                          save_to_dir='dataset/images/b&wZeroPressure/NeckerCubeDrawings/healthy/', save_format='png'):\n",
        "        i += 1\n",
        "        if i > 23:\n",
        "            break \n",
        "            \n",
        "for file in glob.glob(\"dataset/images/b&wZeroPressure/NeckerCubeDrawings/PD/\" + \"*.png\"):\n",
        "    img = load_img(file)  # this is a PIL image\n",
        "    x = img_to_array(img)  # this is a Numpy array with shape (1, 200, 200)\n",
        "    x = x.reshape((1,) + x.shape)  # this is a Numpy array with shape (1, 200, 200)\n",
        "\n",
        "# the .flow() command below generates batches of randomly transformed images\n",
        "# and saves the results to the `preview/` directory\n",
        "    i = 0\n",
        "    for batch in datagen.flow(x, batch_size=1,\n",
        "                          save_to_dir='dataset/images/b&wZeroPressure/NeckerCubeDrawings/PD/', save_format='png'):\n",
        "        i += 1\n",
        "        if i > 11:\n",
        "            break "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-SGlo93oFga",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a1307ab1-e4eb-4706-fca4-ffcdc0ce5a4c"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from matplotlib import pyplot as plt\n",
        "import cv2\n",
        "import random\n",
        "from PIL import Image\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "\n",
        "\n",
        "DATADIR = \"dataset/images/b&wZeroPressure/NeckerCubeDrawings\"\n",
        "\n",
        "# All the categories you want your neural network to detect\n",
        "CATEGORIES = [\"healthy\", \"PD\"]\n",
        "\n",
        "# The size of the images that your neural network will use\n",
        "\n",
        "IMG_SIZE = 200\n",
        "\n",
        "training_data = []\n",
        "\n",
        "def create_training_data():\n",
        "  for file in glob.glob(\"dataset/images/b&wZeroPressure/NeckerCubeDrawings/healthy/\" + \"*.png\"):\n",
        "      img = load_img(file, color_mode = \"grayscale\")  # this is a PIL image\n",
        "      new_image = img_to_array(img)  # this is a Numpy array with shape (200, 200,1)\n",
        "      training_data.append([new_image, 0])\n",
        "  \n",
        "  print(len(training_data))\n",
        "\n",
        "  for file in glob.glob(\"dataset/images/b&wZeroPressure/NeckerCubeDrawings/PD/\" + \"*.png\"):\n",
        "      img = load_img(file, color_mode = \"grayscale\")  # this is a PIL image\n",
        "      new_image = img_to_array(img)  # this is a Numpy array with shape (200, 200,1)\n",
        "      training_data.append([new_image, 1]) # class is 1 \n",
        "\n",
        "\n",
        "#X = np.array(X).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
        "create_training_data()\n",
        "\n",
        "random.shuffle(training_data) #shufles the data so it's not control and patient\n",
        "\n",
        "X = [] #features\n",
        "y = [] #labels\n",
        "#y = np.array(y)\n",
        "\n",
        "for features, label in training_data:\n",
        "\tX.append(features)\n",
        "\ty.append(label)\n",
        "\n",
        "len(X), len(y)"
      ],
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1202\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2211, 2211)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 164
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "icc-RteKO52a"
      },
      "source": [
        "## Define CNN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XJPv7btCO88K",
        "colab": {}
      },
      "source": [
        "def split_dataset(x,y):\n",
        "\n",
        "    # train and test set (90% training, 10% test)\n",
        "    X_train = X[:int(0.9*int(len(X)))]\n",
        "    y_train = y[:int(0.9*int(len(X)))]\n",
        "    X_test = X[int(0.9*int(len(X))):]\n",
        "    y_test = y[int(0.9*int(len(y))):]\n",
        "    \n",
        "    # validation set (90% training, 10% validation)\n",
        "    X_train = X_train[:int(0.9*int(len(X_train)))]\n",
        "    y_train = y_train[:int(0.9*int(len(y_train)))]\n",
        "    \n",
        "    X_val = X_train[int(0.9*int(len(X_train))):]\n",
        "    y_val = y_train[int(0.9*int(len(y_train))):]\n",
        "    \n",
        "    return X_train, y_train, X_test, y_test, X_val, y_val\n",
        "\n",
        "#len(X_val), len(y_val)\n",
        "\n"
      ],
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DbM-Dl4GPBp_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "outputId": "00a876e4-11ae-4014-b65a-3878867a6b25"
      },
      "source": [
        "# To check \n",
        "X_train, y_train, X_test, y_test, X_val, y_val = split_dataset(X,y)\n",
        "\n",
        "X_train, y_train, X_test, y_test, X_val, y_val = prepare_dataset(X_train, y_train, X_test, y_test, X_val, y_val)\n",
        "\n",
        "print(len(X_val), len(y_val),len(X_train), len(y_train), len(X_test), len(y_test))\n"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-165-05940850f993>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-160-4d4dcd1d91a1>\u001b[0m in \u001b[0;36mprepare_dataset\u001b[0;34m(X_train, y_train, X_test, y_test, X_val, y_val)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# reshape training sets to prepare for CNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMG_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMG_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 1790 into shape (200,200,1)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "53LOpuwPPD1e",
        "colab": {}
      },
      "source": [
        "def prepare_dataset(X_train, y_train, X_test, y_test, X_val, y_val):\n",
        "\n",
        "    # reshape training sets to prepare for CNN\n",
        "    X_train = np.array(X_train).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
        "    y_train = np.array(y_train)\n",
        "    \n",
        "    X_test = np.array(X_test).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
        "    y_test = np.array(y_test)\n",
        "    \n",
        "    X_val = np.array(X_val).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
        "    y_val = np.array(y_val)\n",
        "    \n",
        "    # one hot encoding of pixels\n",
        "    y_train = to_categorical(y_train)\n",
        "    y_test = to_categorical(y_test)\n",
        "    y_val = to_categorical(y_val)\n",
        "    \n",
        "    train_norm = X_train.astype('float32')\n",
        "    test_norm = X_test.astype('float32')\n",
        "    val_norm = X_val.astype('float32')\n",
        "\n",
        "    # normalize to range 0-1\n",
        "    X_train = train_norm / 255.0\n",
        "    X_test = test_norm / 255.0\n",
        "    X_val = val_norm / 255.0\n",
        "    # return dataset\n",
        "    \n",
        "    return X_train, y_train, X_test, y_test, X_val, y_val"
      ],
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zANHvFqRYl12",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bf9fa723-f67b-4c99-c22f-428d5121f500"
      },
      "source": [
        "f\n",
        "\n",
        "or i in X_test:\n",
        "  print(i.shape)"
      ],
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(150, 150, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(150, 150, 1)\n",
            "(200, 200, 1)\n",
            "(150, 150, 1)\n",
            "(150, 150, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(150, 150, 1)\n",
            "(200, 200, 1)\n",
            "(150, 150, 1)\n",
            "(200, 200, 1)\n",
            "(150, 150, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(150, 150, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(150, 150, 1)\n",
            "(150, 150, 1)\n",
            "(200, 200, 1)\n",
            "(150, 150, 1)\n",
            "(150, 150, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(150, 150, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(150, 150, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(150, 150, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(150, 150, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(150, 150, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(150, 150, 1)\n",
            "(200, 200, 1)\n",
            "(150, 150, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(150, 150, 1)\n",
            "(150, 150, 1)\n",
            "(200, 200, 1)\n",
            "(150, 150, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(150, 150, 1)\n",
            "(150, 150, 1)\n",
            "(200, 200, 1)\n",
            "(150, 150, 1)\n",
            "(200, 200, 1)\n",
            "(150, 150, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(150, 150, 1)\n",
            "(200, 200, 1)\n",
            "(150, 150, 1)\n",
            "(150, 150, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(150, 150, 1)\n",
            "(150, 150, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(150, 150, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(150, 150, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(150, 150, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(150, 150, 1)\n",
            "(200, 200, 1)\n",
            "(150, 150, 1)\n",
            "(150, 150, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(150, 150, 1)\n",
            "(150, 150, 1)\n",
            "(150, 150, 1)\n",
            "(200, 200, 1)\n",
            "(150, 150, 1)\n",
            "(150, 150, 1)\n",
            "(150, 150, 1)\n",
            "(150, 150, 1)\n",
            "(200, 200, 1)\n",
            "(150, 150, 1)\n",
            "(150, 150, 1)\n",
            "(200, 200, 1)\n",
            "(150, 150, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(150, 150, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(150, 150, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(150, 150, 1)\n",
            "(150, 150, 1)\n",
            "(150, 150, 1)\n",
            "(150, 150, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(150, 150, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(150, 150, 1)\n",
            "(200, 200, 1)\n",
            "(150, 150, 1)\n",
            "(150, 150, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(150, 150, 1)\n",
            "(200, 200, 1)\n",
            "(150, 150, 1)\n",
            "(200, 200, 1)\n",
            "(150, 150, 1)\n",
            "(200, 200, 1)\n",
            "(150, 150, 1)\n",
            "(150, 150, 1)\n",
            "(150, 150, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(150, 150, 1)\n",
            "(150, 150, 1)\n",
            "(200, 200, 1)\n",
            "(150, 150, 1)\n",
            "(150, 150, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(150, 150, 1)\n",
            "(200, 200, 1)\n",
            "(150, 150, 1)\n",
            "(150, 150, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(150, 150, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(150, 150, 1)\n",
            "(150, 150, 1)\n",
            "(150, 150, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(150, 150, 1)\n",
            "(150, 150, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(150, 150, 1)\n",
            "(200, 200, 1)\n",
            "(150, 150, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n",
            "(200, 200, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kP8nJdzePJPF",
        "colab": {}
      },
      "source": [
        "# test harness for evaluating models on the cifar10 dataset\n",
        "import sys\n",
        "from matplotlib import pyplot\n",
        "from keras.datasets import cifar10, mnist\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.optimizers import SGD\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        " \n",
        "# define cnn model\n",
        "def define_model():\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(200, 200, 1)))\n",
        "  model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "  model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Dropout(0.3))\n",
        "  model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "  model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Dropout(0.4))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(2, activation='softmax'))\n",
        "  # compile model\n",
        "  opt = SGD(lr=0.001, momentum=0.9)\n",
        "  model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  print(\"Model defined\")\n",
        "  return model\n",
        " \n",
        "# plot diagnostic learning curves\n",
        "def summarize_diagnostics(history):\n",
        "\t# plot loss\n",
        "\tpyplot.subplot(211)\n",
        "\tpyplot.title('Cross Entropy Loss')\n",
        "\tpyplot.plot(history.history['loss'], color='blue', label='train')\n",
        "\tpyplot.plot(history.history['val_loss'], color='orange', label='test')\n",
        "\t# plot accuracy\n",
        "\tpyplot.subplot(212)\n",
        "\tpyplot.title('Classification Accuracy')\n",
        "\tpyplot.plot(history.history['accuracy'], color='blue', label='train')\n",
        "\tpyplot.plot(history.history['val_accuracy'], color='orange', label='test')\n",
        "    #pyplot.show()\n",
        "\t# save plot to file\n",
        "\tfilename = sys.argv[0].split('/')[-1]\n",
        "\tpyplot.savefig(filename + '_plot.png')\n",
        "\tpyplot.close()\n",
        "    \n",
        " \n",
        "# run the test harness for evaluating a model\n",
        "def run_test_harness(x,y):\n",
        "    \n",
        "    #X_train, y_train, X_test, y_test = create_test(X,y)\n",
        "    \n",
        "    #X_train, y_train, X_val, y_val = create_validation(X_train,y_train)\n",
        "    \n",
        "    X_train, y_train, X_test, y_test, X_val, y_val = split_dataset(X,y)\n",
        "    \n",
        "    X_train, y_train, X_test, Y_test, X_val, y_val = prepare_dataset(X_train, y_train, X_test, y_test, X_val, y_val)\n",
        "    \n",
        "    print(len(X_val), len(y_val))\n",
        "          \n",
        "    model = define_model()\n",
        "    # fit model\n",
        "    history = model.fit(X_train, y_train, epochs=10, batch_size=24, validation_data=(X_val, y_val), verbose=2)\n",
        "    # evaluate model\n",
        "    _, acc = model.evaluate(X_test, Y_test, verbose=1)\n",
        "    print('Test Accuracy: %.3f' % (acc * 100.0))\n",
        "\n",
        "    # Kappa statistic\n",
        "\n",
        "    model.predict(y_test)\n",
        "    yhat_classes = model.predict_classes(testX, verbose=0)\n",
        "    y_values = y_test\n",
        "\n",
        "    kappa = cohen_kappa_score(y_values, yhat_classes)\n",
        "    kappa, y_values\n",
        "\n",
        "    cohen_kappa_score(y_test, predictions)\n",
        "    # learning curves\n",
        "    summarize_diagnostics(history)"
      ],
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QCFXLt1gPNEk",
        "colab": {}
      },
      "source": [
        "run_test_harness(X,y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVrSzCzjfFMF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "outputId": "5277ca15-2c8a-447e-ea5e-93ce3a8c63cb"
      },
      "source": [
        "    model = define_model()\n",
        "\n",
        "    X_train, y_train, X_test, y_test, X_val, y_val = split_dataset(X,y)\n",
        "    X_train, y_train, X_test, Y_test, X_val, y_val = prepare_dataset(X_train, y_train, X_test, y_test, X_val, y_val)\n",
        "\n",
        "\n",
        "    yhat_classes = model.predict_classes(X_test, verbose=1)\n",
        "    #yhat_classes = yhat_classes[:,0]\n",
        "    yhat_classes \n",
        "    y_values = y_test\n",
        "\n",
        "    kappa = cohen_kappa_score(y_values, yhat_classes)\n",
        "    kappa, y_values"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model defined\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-158-aae5791b284d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-150-4d4dcd1d91a1>\u001b[0m in \u001b[0;36mprepare_dataset\u001b[0;34m(X_train, y_train, X_test, y_test, X_val, y_val)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# reshape training sets to prepare for CNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMG_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMG_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 1790 into shape (200,200,1)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EioykrP7jEFR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}