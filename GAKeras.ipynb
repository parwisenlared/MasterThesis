{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAKeras.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMy9g8QemN5QDfleMPRiLeL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parwisenlared/MasterThesis/blob/master/GAKeras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t64fMckuKV_r",
        "colab_type": "text"
      },
      "source": [
        "Code from GAKeras repository, trying to see if it works."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLCcmg5kK1G9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "812b3b0e-9336-410f-a0bd-1184ad567ecc"
      },
      "source": [
        "pip install deap"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: deap in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from deap) (1.18.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_kWy66NKSFi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Algorithm\n",
        "\n",
        "import random\n",
        "import pickle\n",
        "import time\n",
        "import datetime \n",
        "\n",
        "from deap import algorithms \n",
        "\n",
        "\n",
        "def myEASimple(population, start_gen, toolbox, cxpb, mutpb, ngen, \n",
        "               stats, halloffame, logbook, verbose, id=None):\n",
        "\n",
        "    total_time = datetime.timedelta(seconds=0) \n",
        "    for gen in range(start_gen, ngen):\n",
        "        start_time = datetime.datetime.now()\n",
        "        population = algorithms.varAnd(population, toolbox, cxpb=cxpb, mutpb=mutpb)\n",
        "\n",
        "        # Evaluate the individuals with an invalid fitness\n",
        "        invalid_ind = [ind for ind in population if not ind.fitness.valid]\n",
        "        fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
        "        for ind, fit in zip(invalid_ind, fitnesses):\n",
        "            ind.fitness.values = fit\n",
        "\n",
        "        halloffame.update(population)\n",
        "        record = stats.compile(population)\n",
        "        logbook.record(gen=gen, evals=len(invalid_ind), **record)\n",
        "        if verbose:\n",
        "            print(logbook.stream)\n",
        "\n",
        "\n",
        "        population = toolbox.select(population, k=len(population))\n",
        "\n",
        "        if gen % 1 == 0:\n",
        "            # Fill the dictionary using the dict(key=value[, ...]) constructor\n",
        "            cp = dict(population=population, generation=gen, halloffame=halloffame,\n",
        "                      logbook=logbook, rndstate=random.getstate())\n",
        "            if id is None:\n",
        "                cp_name = \"checkpoint_ea.pkl\"\n",
        "            else:\n",
        "                cp_name = \"checkpoint_ea_{}.pkl\".format(id)\n",
        "            pickle.dump(cp, open(cp_name, \"wb\"))\n",
        "            \n",
        "        gen_time = datetime.datetime.now() - start_time\n",
        "        total_time = total_time + gen_time\n",
        "        #print(\"Time \", total_time)\n",
        "        if total_time > datetime.timedelta(hours=4*24):\n",
        "            print(\"Time limit exceeded.\")\n",
        "            break \n",
        "\n",
        "    return population, logbook\n",
        "\n",
        "\n",
        "def myEAMuCommaLambda(population, startgen, toolbox, mu, lambda_, cxpb, mutpb, ngen,\n",
        "                      stats=None, halloffame=None, logbook=None, verbose=False, id=None):\n",
        "\n",
        "    assert lambda_ >= mu, \"lambda must be greater or equal to mu.\"\n",
        "\n",
        "    # Evaluate the individuals with an invalid fitness\n",
        "    invalid_ind = [ind for ind in population if not ind.fitness.valid]\n",
        "    fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
        "    for ind, fit in zip(invalid_ind, fitnesses):\n",
        "        ind.fitness.values = fit\n",
        "\n",
        "    if halloffame is not None:\n",
        "        halloffame.update(population)\n",
        "\n",
        "    if logbook is None:\n",
        "        logbook = tools.Logbook()\n",
        "        logbook.header = ['gen', 'nevals'] + (stats.fields if stats else [])\n",
        "\n",
        "    record = stats.compile(population) if stats is not None else {}\n",
        "    logbook.record(gen=startgen, nevals=len(invalid_ind), **record)\n",
        "    if verbose:\n",
        "        print(logbook.stream)\n",
        "\n",
        "    # Begin the generational process\n",
        "    total_time = datetime.timedelta(seconds=0) \n",
        "    for gen in range(startgen+1, ngen):\n",
        "        start_time = datetime.datetime.now()\n",
        "        \n",
        "        # Vary the population\n",
        "        offspring = algorithms.varOr(population, toolbox, lambda_, cxpb, mutpb)\n",
        "\n",
        "        # Evaluate the individuals with an invalid fitness\n",
        "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
        "        fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
        "        for ind, fit in zip(invalid_ind, fitnesses):\n",
        "            ind.fitness.values = fit\n",
        "\n",
        "        # Update the hall of fame with the generated individuals\n",
        "        if halloffame is not None:\n",
        "            halloffame.update(offspring)\n",
        "\n",
        "        # Select the next generation population\n",
        "        population[:] = toolbox.select(offspring, mu)\n",
        "\n",
        "        # Update the statistics with the new population\n",
        "        record = stats.compile(population) if stats is not None else {}\n",
        "        logbook.record(gen=gen, nevals=len(invalid_ind), **record)\n",
        "        if verbose:\n",
        "            print(logbook.stream)\n",
        "\n",
        "        if gen % 1 == 0:\n",
        "            # Fill the dictionary using the dict(key=value[, ...]) constructor\n",
        "            cp = dict(population=population, generation=gen, halloffame=halloffame,\n",
        "                      logbook=logbook, rndstate=random.getstate())\n",
        "            if id is None:\n",
        "                cp_name = \"checkpoint_es.pkl\"\n",
        "            else:\n",
        "                cp_name = \"checkpoint_es_{}.pkl\".format(id)\n",
        "            pickle.dump(cp, open(cp_name, \"wb\"))\n",
        "\n",
        "        gen_time = datetime.datetime.now() - start_time\n",
        "        total_time = total_time + gen_time\n",
        "        if total_time > datetime.timedelta(hours=4*24):\n",
        "            print(\"Time limit exceeded.\")\n",
        "            break \n",
        "\n",
        "            \n",
        "            \n",
        "    return population, logbook"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kazLrhLKc5D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Configuration MNIST\n",
        "\n",
        "class Cfg:\n",
        "    pass \n",
        "\n",
        "class CfgSensors:\n",
        "\n",
        "    batch_size = 100\n",
        "    epochs = 500\n",
        "    loss = 'mean_squared_error'\n",
        "\n",
        "    task_type = 'approximation'\n",
        "\n",
        "    pop_size = 10\n",
        "    ngen = 30\n",
        "\n",
        "    MAX_LAYERS = 5\n",
        "    MAX_LAYER_SIZE = 100\n",
        "    MIN_LAYER_SIZE = 5\n",
        "    DROPOUT = [ 0.0, 0.2, 0.3, 0.4 ] \n",
        "    ACTIVATIONS = [ 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear' ] \n",
        "\n",
        "\n",
        "class CfgSensorsES:\n",
        "\n",
        "    batch_size = 100\n",
        "    epochs = 500\n",
        "    loss = 'mean_squared_error'\n",
        "\n",
        "    task_type = 'approximation'\n",
        "\n",
        "    MU = 10\n",
        "    LAMBDA = 30\n",
        "    ngen = 100\n",
        "\n",
        "    MAX_LAYERS = 5\n",
        "    MAX_LAYER_SIZE = 100\n",
        "    MIN_LAYER_SIZE = 5\n",
        "    DROPOUT = [ 0.0, 0.2, 0.3, 0.4 ] \n",
        "    ACTIVATIONS = [ 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear' ] \n",
        "\n",
        "    SIZE_MIN_STRATEGY = 5\n",
        "    SIZE_MAX_STRATEGY = 50\n",
        "    DROPOUT_MIN_STRATEGY = 0.05\n",
        "    DROPOUT_MAX_STRATEGY = 0.2\n",
        "\n",
        "    \n",
        "    \n",
        "class CfgMnist:\n",
        "\n",
        "    batch_size = 128\n",
        "    epochs = 10\n",
        "    loss = 'categorical_crossentropy'\n",
        "    #loss = 'mean_squared_error'\n",
        "    \n",
        "    task_type = \"classification\"\n",
        "\n",
        "    pop_size = 20\n",
        "    ngen = 300\n",
        "\n",
        "    MAX_LAYERS = 5\n",
        "    MAX_LAYER_SIZE = 300\n",
        "    MIN_LAYER_SIZE = 10 \n",
        "    DROPOUT = [ 0.0, 0.2, 0.3, 0.4 ] \n",
        "    ACTIVATIONS = [ 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear' ] \n",
        "\n",
        "    # for convolutional networks\n",
        "    MIN_FILTERS = 10\n",
        "    MAX_FILTERS = 50 \n",
        "    MIN_KERNEL_SIZE = 2\n",
        "    MAX_KERNEL_SIZE = 5\n",
        "    MIN_POOL_SIZE = 2\n",
        "    MAX_POOL_SIZE = 3\n",
        "    MAX_CONV_LAYERS = 3\n",
        "    MAX_DENSE_LAYERS = 3\n",
        "    \n",
        "    #DENSE_LAYER = 0.5\n",
        "    CONV_LAYER = 0.7 \n",
        "    MAX_POOL_LAYER = 0.3 \n",
        "\n",
        "    \n",
        "    \n",
        "class CfgMnistES:\n",
        "\n",
        "    batch_size = 128\n",
        "    epochs = 20 \n",
        "    #loss = 'categorical_crossentropy'\n",
        "    loss = 'mean_squared_error'\n",
        "    \n",
        "    task_type = \"classification\"\n",
        "\n",
        "    MU = 5\n",
        "    LAMBDA = 10\n",
        "    ngen = 10\n",
        "\n",
        "    SIZE_MIN_STRATEGY = 5\n",
        "    SIZE_MAX_STRATEGY = 50\n",
        "    DROPOUT_MIN_STRATEGY = 0.05\n",
        "    DROPOUT_MAX_STRATEGY = 0.2\n",
        "    \n",
        "    MAX_LAYERS = 5\n",
        "    MAX_LAYER_SIZE = 1000\n",
        "    MIN_LAYER_SIZE = 10 \n",
        "    DROPOUT = [ 0.0, 0.2, 0.3, 0.4 ] \n",
        "    ACTIVATIONS = [ 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear' ] \n",
        "\n",
        "#Config = CfgSensors()    \n",
        "Config = CfgMnist()\n",
        "#Config = CfgSensorsES()  \n",
        "#Config = CfgMnist()\n",
        "\n",
        "import configparser\n",
        "import re\n",
        "\n",
        "def is_int(s):\n",
        "    if re.fullmatch(r'[0-9]+', s):\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "def is_float(s):\n",
        "    if re.fullmatch(r'[0-9]+\\.[0-9]+', s):\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "def is_list(s):\n",
        "    if re.fullmatch(r'\\[.+\\]', s):\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "def convert(s):\n",
        "    if is_int(s):\n",
        "        val = int(s)\n",
        "    elif is_float(s):\n",
        "        val = float(s)\n",
        "    elif is_list(s):\n",
        "        s = s.strip()\n",
        "        s = s.strip(\"[\")\n",
        "        s = s.strip(\"]\")\n",
        "        val = s.split(',')\n",
        "        newval = [] \n",
        "        for v in val:\n",
        "            v = v.strip()\n",
        "            v = v.strip(\"'\")\n",
        "            v = v.strip('\"')\n",
        "            newval.append(convert(v))\n",
        "        val = newval\n",
        "    else:\n",
        "        val = s\n",
        "    return val\n",
        "    \n",
        "    \n",
        "def load_config(name):\n",
        "\n",
        "    config = configparser.ConfigParser()\n",
        "    config.read(name)\n",
        "    \n",
        "    global Config\n",
        "    Config = Cfg()\n",
        "    \n",
        "    for sec in config.sections():\n",
        "        for key, val in config[sec].items():\n",
        "            val = convert(val)\n",
        "            setattr(Config, key.lower(), val)\n",
        "            setattr(Config, key.upper(), val)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pQsEGzULePh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Utils\n",
        "\n",
        "import random \n",
        "\n",
        "\n",
        "def roulette(functions, probs):\n",
        "    r = random.random()\n",
        "    for func, prob in zip(functions, probs):\n",
        "        if r < prob:\n",
        "            return func\n",
        "        else:\n",
        "            r -= prob\n",
        "    return None\n",
        "                \n",
        "import numpy as np\n",
        "\n",
        "def mean_sq_error(y1, y2):\n",
        "    diff = y1 - y2\n",
        "    E = 100 * sum(diff*diff) / len(y1)\n",
        "    return E\n",
        "\n",
        "def accuracy_score(y1, y2):\n",
        "    assert y1.shape == y2.shape\n",
        "    \n",
        "    y1_argmax = np.argmax(y1, axis=1)\n",
        "    y2_argmax = np.argmax(y2, axis=1)\n",
        "    score = sum(y1_argmax == y2_argmax)\n",
        "    return score / len(y1)\n",
        "        \n",
        "    \n",
        "def error(y1, y2):\n",
        "    if Config.task_type == \"classification\":\n",
        "        return accuracy_score(y1, y2)\n",
        "    else:\n",
        "        return mean_sq_error(y1, y2)\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufCRsw_5McY3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "26b17031-1520-4b91-82fc-b41128ef57c0"
      },
      "source": [
        "# Individual \n",
        "\n",
        "import random\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "#from config import Config\n",
        "\n",
        "\n",
        "class Layer:\n",
        "    \"\"\" Specification of one layer.\n",
        "        Includes size, regularization, activaton. \n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def randomInit(self):\n",
        "        self.size = random.randint(Config.MIN_LAYER_SIZE, Config.MAX_LAYER_SIZE)\n",
        "        self.dropout = random.choice(Config.DROPOUT) \n",
        "        self.activation = random.choice(Config.ACTIVATIONS)  \n",
        "        return self\n",
        "\n",
        "    def __str__(self):\n",
        "        return \" #{} dropout={} activation={}\".format(self.size, self.dropout, self.activation)\n",
        "\n",
        "\n",
        "class Individual:\n",
        "    \"\"\"  Individual coding network architecture. \n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.input_shape = Config.input_shape\n",
        "        self.noutputs = Config.noutputs \n",
        "        #print(self.input_shape, self.noutputs)\n",
        "\n",
        "        \n",
        "    def randomInit(self):\n",
        "        self.layers = []\n",
        "        num_layers = random.randint(1, Config.MAX_LAYERS)\n",
        "        for l in range(num_layers):\n",
        "            layer = Layer().randomInit() \n",
        "            self.layers.append(layer)\n",
        "\n",
        "    def createNetwork(self):\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        firstlayer = True\n",
        "        for l in self.layers:\n",
        "            if firstlayer:\n",
        "                model.add(Dense(l.size, input_shape=self.input_shape))\n",
        "                firstlayer = False\n",
        "            else:\n",
        "                model.add(Dense(l.size))\n",
        "            model.add(Activation(l.activation))\n",
        "            if l.dropout > 0:\n",
        "                model.add(Dropout(l.dropout))\n",
        "\n",
        "        # final part \n",
        "        model.add(Dense(self.noutputs))\n",
        "        if Config.task_type == \"classification\":\n",
        "            model.add(Activation('softmax'))\n",
        "            \n",
        "        model.compile(loss=Config.loss,\n",
        "                      optimizer=RMSprop())\n",
        "        \n",
        "        return model \n",
        "\n",
        "    def __str__(self):\n",
        "\n",
        "        ret = \"------------------------\\n\"\n",
        "        for l in self.layers:\n",
        "            ret += str(l)\n",
        "            ret += \"\\n\" \n",
        "            ret += \"------------------------\\n\"\n",
        "        return ret \n",
        "\n",
        "def initIndividual(indclass):\n",
        "    ind = indclass()\n",
        "    ind.randomInit()\n",
        "    return ind"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlSWyeLLLDt5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Configuration individual\n",
        "\n",
        "import random\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.optimizers import RMSprop\n",
        "#from individual import Layer\n",
        "\n",
        "class ConvLayer:\n",
        "    \"\"\" Specification of one convolutional layer.\n",
        "        Includes number of filters, kernel size, activation.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def randomInit(self):\n",
        "        self.filters = random.randint(Config.MIN_FILTERS, Config.MAX_FILTERS)\n",
        "        # filters are squares kernel_size x kernel_size \n",
        "        self.kernel_size = random.randint(Config.MIN_KERNEL_SIZE, Config.MAX_KERNEL_SIZE)\n",
        "        self.activation = random.choice(Config.ACTIVATIONS)\n",
        "        return self\n",
        "    \n",
        "    def __str__(self):\n",
        "        return \"conv #{} kernelsize={} activation={}\".format(self.filters, self.kernel_size, self.activation)\n",
        "        \n",
        "\n",
        "class MaxPoolLayer:\n",
        "    \"\"\" Specification of one max pooling layer.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    \n",
        "    def randomInit(self):\n",
        "        # pooling size is (pool_size, pool_size)\n",
        "        self.pool_size =  random.randint(Config.MIN_POOL_SIZE, Config.MAX_POOL_SIZE)\n",
        "        return self\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"pool poolsize={} \".format(self.pool_size)\n",
        "\n",
        "\n",
        "def createRandomLayer():\n",
        "\n",
        "    create = roulette([lambda: ConvLayer(),\n",
        "                       lambda: MaxPoolLayer()],\n",
        "                      [Config.CONV_LAYER, Config.MAX_POOL_LAYER])\n",
        "    if create:\n",
        "        return create()\n",
        "    else:\n",
        "        return ConvLayer() \n",
        "    \n",
        "class ConvIndividual:\n",
        "    \"\"\" Individual coding convolutional network architecture.\n",
        "        Individual consists of two parts, first of convolutioanal\n",
        "        and max pooling layers, the second part of dense layers.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.input_shape = Config.input_shape\n",
        "        self.noutputs = Config.noutputs\n",
        "        self.nparams = None\n",
        "        \n",
        "    def randomInit(self):\n",
        "        self.conv_layers = []\n",
        "        num_conv_layers = random.randint(1, Config.MAX_CONV_LAYERS)\n",
        "        for l in range(num_conv_layers):\n",
        "            layer = createRandomLayer().randomInit()\n",
        "            self.conv_layers.append(layer)\n",
        "            \n",
        "        self.dense_layers = []\n",
        "        num_dense_layers = random.randint(1, Config.MAX_DENSE_LAYERS)\n",
        "        for l in range(num_dense_layers):\n",
        "            layer = Layer().randomInit()\n",
        "            self.dense_layers.append(layer)\n",
        "\n",
        "\n",
        "    def createNetwork(self):\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "\n",
        "        firstlayer = True\n",
        "\n",
        "        # convolutional part \n",
        "        for l in self.conv_layers:\n",
        "            if type(l) is ConvLayer:\n",
        "                if firstlayer:\n",
        "                    model.add(Conv2D(l.filters, (l.kernel_size, l.kernel_size),\n",
        "                                     padding='same', # let not the shape vanish\n",
        "                                     input_shape=self.input_shape))\n",
        "                    firstlayer = False\n",
        "                else:\n",
        "                    model.add(Conv2D(l.filters, (l.kernel_size, l.kernel_size), padding='same'))\n",
        "                model.add(Activation(l.activation))\n",
        "                \n",
        "            elif type(l) is MaxPoolLayer:\n",
        "                if firstlayer:\n",
        "                    model.add(MaxPooling2D(pool_size=(l.pool_size,l.pool_size),\n",
        "                                           input_shape=self.input_shape))\n",
        "                    firstlayer = False\n",
        "                else:\n",
        "                    # check if pooling is possible\n",
        "                    if model.layers[-1].output_shape[1] >= l.pool_size and model.layers[-1].output_shape[2] >= l.pool_size:\n",
        "                        model.add(MaxPooling2D(pool_size=(l.pool_size, l.pool_size)))\n",
        "                    \n",
        "            else:\n",
        "                raise TypeError(\"unknown type of layer\") \n",
        "            \n",
        "        # dense part\n",
        "        model.add(Flatten())\n",
        "        for l in self.dense_layers:\n",
        "            model.add(Dense(l.size))\n",
        "            model.add(Activation(l.activation))\n",
        "            if l.dropout > 0:\n",
        "                model.add(Dropout(l.dropout))\n",
        "\n",
        "        # final part \n",
        "        model.add(Dense(self.noutputs))\n",
        "        if Config.task_type == \"classification\":\n",
        "            model.add(Activation('softmax'))\n",
        "        \n",
        "        model.compile(loss=Config.loss,\n",
        "                      optimizer=RMSprop())\n",
        "\n",
        "        self.nparams = model.count_params()\n",
        "                \n",
        "        return model \n",
        "\n",
        "    \n",
        "    def __str__(self):\n",
        "\n",
        "        ret = \"------------------------\\n\"\n",
        "        for l in self.conv_layers+self.dense_layers:\n",
        "            ret += str(l)\n",
        "            ret += \"\\n\" \n",
        "            ret += \"------------------------\\n\"\n",
        "        return ret "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2U6ffmdzLX9Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Configuration configuration\n",
        "\n",
        "import configparser\n",
        "\n",
        "config = configparser.ConfigParser()\n",
        "\n",
        "config['DEFAULT'] = { 'batch_size': 128,\n",
        "                      'epochs': 20,\n",
        "                      'loss': 'mean_squared_error',\n",
        "                      'task_type': 'classification' }\n",
        "\n",
        "config['ES'] = { 'mu': 5,\n",
        "                 'lambda': 10,\n",
        "                 'ngen': 20 }\n",
        "\n",
        "config['STRATEGY_COEFS'] = { 'size_min_strategy': 5,\n",
        "                             'size_max_strategy': 50,\n",
        "                             'dropout_min_strategy': 0.05,\n",
        "                             'dropout_max_strategy': 0.2 }\n",
        "\n",
        "config['NETWORK'] = {'max_layers': 5,\n",
        "                     'max_layer_size': 1000,\n",
        "                     'min_layer_size': 10,\n",
        "                     'dropout': [ 0.0, 0.2, 0.3, 0.4 ],\n",
        "                     'activations': [ 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear' ] }\n",
        "\n",
        "with open('example.ini', 'w') as configfile:\n",
        "    config.write(configfile)\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEQKn-YIMzVv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Crossover\n",
        "\n",
        "import random \n",
        "\n",
        "def cxListOnePoint(list1, list2):\n",
        "    assert len(list1) > 0 and len(list2) > 0\n",
        "    \n",
        "    cxpoint1 = random.randint(0, len(list1) - 1)\n",
        "    cxpoint2 = random.randint(0, len(list2) - 1)\n",
        "    list1[cxpoint1:], list2[cxpoint2:] = list2[cxpoint2:], list1[cxpoint1:]\n",
        "    \n",
        "    assert len(list1) > 0 and len(list2) > 0 \n",
        "    \n",
        "\n",
        "class Crossover:\n",
        "\n",
        "    def cxOnePoint(self, ind1, ind2):\n",
        "        #do one-point crossover on list of layers \n",
        "        cxListOnePoint(ind1.layers, ind2.layers) \n",
        "        return ind1, ind2 \n",
        "\n",
        "    \n",
        "class CrossoverConv:\n",
        "\n",
        "    def cxOnePoint(self, ind1, ind2):\n",
        "        #do one-point crossover on list of layers\n",
        "        #convolutional layers\n",
        "        cxListOnePoint(ind1.conv_layers, ind2.conv_layers)\n",
        "        #dense layers \n",
        "        cxListOnePoint(ind1.dense_layers, ind2.dense_layers)\n",
        "        \n",
        "        return ind1, ind2 \n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nKNRdSkM3kE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Crossover ES\n",
        "\n",
        "import random \n",
        "\n",
        "class CrossoverES:\n",
        "\n",
        "    def cxOnePoint(self, ind1, ind2):\n",
        "        #do one-point crossover on list of layers \n",
        "        cxpoint1 = random.randint(0, len(ind1.layers) - 1)\n",
        "        cxpoint2 = random.randint(0, len(ind2.layers) - 1)\n",
        "\n",
        "        ind1.layers[cxpoint1:], ind2.layers[cxpoint2:] = ind2.layers[cxpoint2:], ind1.layers[cxpoint1:]\n",
        "\n",
        "        s1, s2  = ind1.strategy, ind2.strategy\n",
        "        s1.blocks[cxpoint1:], s2.blocks[cxpoint2:] = s2.blocks[cxpoint2:], s1.blocks[cxpoint1:] \n",
        "        \n",
        "        assert len(ind1.layers) > 0\n",
        "        assert len(ind2.layers) > 0 \n",
        "        assert len(ind1.layers) == len(ind1.strategy.blocks)\n",
        "        assert len(ind2.layers) == len(ind2.strategy.blocks)\n",
        "        \n",
        "        return ind1, ind2 "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mi41vOE_M64S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dataset \n",
        "import sys\n",
        "from matplotlib import pyplot\n",
        "from keras.datasets import cifar10, mnist\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "def load_dataset():\n",
        "\t# load dataset\n",
        "  (trainX, trainY), (testX, testY) = mnist.load_data()\n",
        "\t# reshape dataset to have a single channel\n",
        "  X_train = trainX.reshape((trainX.shape[0], 28, 28, 1))\n",
        "  X_test = testX.reshape((testX.shape[0], 28, 28, 1))\n",
        "\t# one hot encode target values\n",
        "  Y_train = to_categorical(trainY)\n",
        "  Y_test = to_categorical(testY)\n",
        "  print(\"Data loaded\")\n",
        "  print(\"Preparing pixels\")\n",
        "  # convert from integers to floats\n",
        "  train_norm = X_train.astype('float32')\n",
        "  test_norm = X_test.astype('float32')\n",
        "  # normalize to range 0-1\n",
        "  X_train = train_norm / 255.0\n",
        "  X_test = test_norm / 255.0\n",
        "  # return normalized images\n",
        "  print(\"Pixel data ready\")\n",
        "\n",
        "  return X_train, Y_train, X_test, Y_test"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2ZZSF-fNJxE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fitness \n",
        "\n",
        "import random \n",
        "import numpy as np \n",
        "import pickle\n",
        "import sklearn\n",
        "from sklearn.model_selection import KFold\n",
        "from keras import backend as K \n",
        "\n",
        "\n",
        "class Database:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.data = []\n",
        "    \n",
        "    def insert(self, individual, fitness):\n",
        "        self.data.append((individual, fitness))\n",
        "        \n",
        "    def save(self, name):\n",
        "        with open(name, \"wb\") as f:\n",
        "            pickle.dump(self.data, f)\n",
        "\n",
        "class Fitness:\n",
        "\n",
        "    def __init__(self, xtrain, ytrain):\n",
        "        \n",
        "        # load train data \n",
        "        self.X = xtrain\n",
        "        self.y = ytrain\n",
        "                \n",
        "    def evaluate(self, individual):\n",
        "        #print(\" *** evaluate *** \")\n",
        "\n",
        "        #model = individual.createNetwork()\n",
        "        #return random.random(), \n",
        "         \n",
        "        random.seed(42) \n",
        "        # perform KFold crossvalidation \n",
        "        kf = KFold(n_splits=5)\n",
        "        scores = []\n",
        "        for train, test in kf.split(self.X):   # train, test are indicies \n",
        "            X_train, X_test = self.X[train], self.X[test]\n",
        "            y_train, y_test = self.y[train], self.y[test]\n",
        "                \n",
        "            model = individual.createNetwork()\n",
        "            model.fit(X_train, y_train,\n",
        "                      batch_size=Config.batch_size, epochs=Config.epochs, verbose=0)\n",
        "            \n",
        "            yy_test = model.predict(X_test)\n",
        "            scores.append(error(y_test, yy_test))\n",
        "            \n",
        "        fitness = np.mean(scores)\n",
        "            \n",
        "        # I try this to prevent memory leaks in nsga2-keras \n",
        "        K.clear_session()\n",
        "\n",
        "        return fitness"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kAAS_RVR_xH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mutation for NETWORK\n",
        "\n",
        "import random\n",
        "\n",
        "PROB_MUTATE_LAYER = 0.5\n",
        "PROB_ADD_LAYER = 0.25\n",
        "PROB_DEL_LAYER = 0.25\n",
        "\n",
        "PROB_CHANGE_LAYER_SIZE = 0.4\n",
        "PROB_CHANGE_DROPOUT = 0.25\n",
        "PROB_CHANGE_ACTIVATION = 0.25\n",
        "PROB_CHANGE_ALL = 0.1 \n",
        "\n",
        "PROB_RANDOM_LAYER_SIZE = 0.4\n",
        "PROB_ADD_NEURON = 0.3\n",
        "PROB_DEL_NEURON = 0.3 \n",
        "\n",
        "class Mutation:\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "    def changeSize(self, layer):\n",
        "        layer.size = random.randint(Config.MIN_LAYER_SIZE, Config.MAX_LAYER_SIZE)\n",
        "\n",
        "    def addNeuron(self, layer):\n",
        "        layer.size += 1\n",
        "\n",
        "    def delNeuron(self, layer):\n",
        "        if layer.size > 0:\n",
        "            layer.size -= 1 \n",
        "    \n",
        "    def mutateSize(self, layer):\n",
        "\n",
        "        mutfunc = roulette([self.changeSize, self.addNeuron, self.delNeuron],\n",
        "                           [PROB_RANDOM_LAYER_SIZE, PROB_ADD_NEURON, PROB_DEL_NEURON])\n",
        "        if mutfunc:\n",
        "            mutfunc(layer)\n",
        "\n",
        "\n",
        "    def mutateDropout(self, layer):\n",
        "        layer.dropout = random.choice(Config.DROPOUT)\n",
        "        \n",
        "    def mutateActivation(self, layer):\n",
        "        layer.activation = random.choice(Config.ACTIVATIONS)\n",
        "\n",
        "    def randomInit(self, layer):\n",
        "        layer.randomInit()\n",
        "\n",
        "    def mutateLayer(self, individual):\n",
        "\n",
        "        # select layer random \n",
        "        l = random.randint(0, len(individual.layers)-1)\n",
        "        layer = individual.layers[l]\n",
        "\n",
        "        mutfunc = roulette([self.mutateSize, self.mutateDropout, self.mutateActivation,\n",
        "                            self.randomInit],\n",
        "                           [PROB_CHANGE_LAYER_SIZE, PROB_CHANGE_DROPOUT, PROB_CHANGE_ACTIVATION,\n",
        "                            PROB_CHANGE_ALL])\n",
        "\n",
        "        if mutfunc:\n",
        "            mutfunc(layer)\n",
        "        \n",
        "        return individual,\n",
        "\n",
        "    def addLayer(self, individual):\n",
        "        l = random.randint(0, len(individual.layers)-1)\n",
        "        individual.layers.insert(l, Layer().randomInit())\n",
        "        return individual, \n",
        "\n",
        "    def delLayer(self, individual):\n",
        "        if len(individual.layers)>1:\n",
        "            l = random.randint(0, len(individual.layers)-1)\n",
        "            del individual.layers[l]\n",
        "        return individual, \n",
        "    \n",
        "    def mutate(self, individual): \n",
        "        \n",
        "        mutfunc = roulette([self.mutateLayer, self.addLayer, self.delLayer],\n",
        "                           [PROB_MUTATE_LAYER, PROB_ADD_LAYER, PROB_DEL_LAYER])\n",
        "        if mutfunc:\n",
        "            return mutfunc(individual)\n",
        "        \n",
        "        return individual, \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Mutation in COnv Layers\n",
        "PROB_CHANGE_NUM_FILTERS = 0.3\n",
        "PROB_MUTATE_KERNEL_SIZE = 0.3\n",
        "PROB_CHANGE_CONV_ACTIVATION = 0.3\n",
        "PROB_CHANGE_CONV_ALL = 0.1\n",
        "\n",
        "\n",
        "\n",
        "class MutationConv(Mutation):\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "    def mutateFilters(self, layer):\n",
        "        layer.filters = random.randint(Config.MIN_FILTERS, Config.MAX_FILTERS)\n",
        "\n",
        "    def mutateKernelSize(self, layer):\n",
        "        layer.kernel_size = random.randint(Config.MIN_KERNEL_SIZE, Config.MAX_KERNEL_SIZE)\n",
        "\n",
        "    def mutatePoolSize(self, layer):\n",
        "        layer.pool_size =  random.randint(Config.MIN_POOL_SIZE, Config.MAX_POOL_SIZE)\n",
        "        \n",
        "        \n",
        "    def mutateLayer(self, individual):\n",
        "\n",
        "        layers = individual.conv_layers + individual.dense_layers\n",
        "        # select layer random \n",
        "        l = random.randint(0, len(layers)-1)\n",
        "        layer = layers[l]\n",
        "\n",
        "        if type(layer) is Layer:\n",
        "            mutfunc = roulette([self.mutateSize, self.mutateDropout, self.mutateActivation,\n",
        "                                self.randomInit],\n",
        "                               [PROB_CHANGE_LAYER_SIZE, PROB_CHANGE_DROPOUT, PROB_CHANGE_ACTIVATION,\n",
        "                               PROB_CHANGE_ALL])\n",
        "            if mutfunc:\n",
        "                mutfunc(layer)\n",
        "        elif type(layer) is ConvLayer:\n",
        "            mutfunc = roulette([self.mutateFilters, self.mutateKernelSize, self.mutateActivation,\n",
        "                                self.randomInit],\n",
        "                               [PROB_CHANGE_NUM_FILTERS, PROB_MUTATE_KERNEL_SIZE, PROB_CHANGE_CONV_ACTIVATION,\n",
        "                                PROB_CHANGE_CONV_ALL])\n",
        "            if mutfunc:\n",
        "                mutfunc(layer)\n",
        "        elif type(layer) is MaxPoolLayer:\n",
        "            self.mutatePoolSize(layer)\n",
        "        else:\n",
        "            raise(TypeError(\"unknown type of layer\"))\n",
        "            \n",
        "                \n",
        "        return individual,\n",
        "\n",
        "    def addLayer(self, individual):\n",
        "        conv_part = random.randint(0,1)\n",
        "        if (conv_part):\n",
        "            l = random.randint(0, len(individual.conv_layers)-1)\n",
        "            if random.randint(0,1):\n",
        "                individual.conv_layers.insert(l, ConvLayer().randomInit())\n",
        "            else:\n",
        "                individual.conv_layers.insert(l, MaxPoolLayer().randomInit())\n",
        "        else:\n",
        "            l = random.randint(0, len(individual.dense_layers)-1)\n",
        "            individual.dense_layers.insert(l, Layer().randomInit())\n",
        "            \n",
        "        return individual, \n",
        "\n",
        "    def delLayer(self, individual):\n",
        "        conv_part = random.randint(0,1)\n",
        "        if (conv_part):\n",
        "            if len(individual.conv_layers)>1:\n",
        "                l = random.randint(0, len(individual.conv_layers)-1)\n",
        "                del individual.conv_layers[l]\n",
        "        else:\n",
        "            if len(individual.dense_layers)>1:\n",
        "                l = random.randint(0, len(individual.dense_layers)-1)\n",
        "                del individual.dense_layers[l]\n",
        "                \n",
        "        return individual,\n",
        "    \n",
        "    \n",
        "    def mutate(self, individual): \n",
        "        \n",
        "        mutfunc = roulette([self.mutateLayer, self.addLayer, self.delLayer],\n",
        "                           [PROB_MUTATE_LAYER, PROB_ADD_LAYER, PROB_DEL_LAYER])\n",
        "        if mutfunc:\n",
        "            return mutfunc(individual)\n",
        "        \n",
        "        return individual, \n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbESws44Se4C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mutation ES\n",
        "\n",
        "import random\n",
        "import math\n",
        "\n",
        "\n",
        "PROB_MUTATE_LAYER = 0.5\n",
        "PROB_ADD_LAYER = 0.25\n",
        "PROB_DEL_LAYER = 0.25\n",
        "\n",
        "PROB_CHANGE_LAYER_SIZE = 0.4\n",
        "PROB_CHANGE_DROPOUT = 0.3\n",
        "PROB_CHANGE_ACTIVATION = 0.3\n",
        "\n",
        "class MutationES:\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "    def mutateSize(self, layer, strategy, size):\n",
        "        # strategy coeficients \n",
        "        t = 1.0 / math.sqrt(2. * math.sqrt(size))\n",
        "        t0 = 1.0 / math.sqrt(2. * size)\n",
        "        n = random.gauss(0, 1)\n",
        "        t0_n = t0 * n\n",
        "        # modify strategy\n",
        "        strategy.size *= math.exp(t0_n + t * random.gauss(0, 1))\n",
        "        # modify size \n",
        "        layer.size += round(strategy.size * random.gauss(0, 1))\n",
        "        layer.size = int(layer.size)\n",
        "        if layer.size < Config.MIN_LAYER_SIZE:\n",
        "            layer.size = Config.MIN_LAYER_SIZE\n",
        "        if layer.size > Config.MAX_LAYER_SIZE:\n",
        "            layer.size = Config.MAX_LAYER_SIZE\n",
        "\n",
        "    def mutateDropout(self, layer, strategy, size):\n",
        "        # strategy coeficients \n",
        "        t = 1.0 / math.sqrt(2. * math.sqrt(size))\n",
        "        t0 = 1.0 / math.sqrt(2. * size)\n",
        "        n = random.gauss(0, 1)\n",
        "        t0_n = t0 * n\n",
        "        # modify strategy\n",
        "        strategy.dropout *= math.exp(t0_n + t * random.gauss(0, 1))\n",
        "        # modify dropout \n",
        "        layer.dropout += strategy.dropout * random.gauss(0, 1)\n",
        "        if layer.dropout < 0.0:\n",
        "            layer.dropout = 0\n",
        "        if layer.dropout > 0.9:\n",
        "            layer.dropout = 0.9 \n",
        "        \n",
        "    def mutateActivation(self, layer, strategy, size):\n",
        "        layer.activation = random.choice(Config.ACTIVATIONS)\n",
        "\n",
        "\n",
        "    def mutateLayer(self, individual):\n",
        "\n",
        "        # select layer random \n",
        "        l = random.randint(0, len(individual.layers)-1)\n",
        "        layer = individual.layers[l]\n",
        "        strategy = individual.strategy.blocks[l]\n",
        "        \n",
        "        mutfunc = roulette([self.mutateSize, self.mutateDropout, self.mutateActivation],\n",
        "                           [PROB_CHANGE_LAYER_SIZE, PROB_CHANGE_DROPOUT, PROB_CHANGE_ACTIVATION])\n",
        "\n",
        "        if mutfunc:\n",
        "            mutfunc(layer, strategy, len(individual.layers))\n",
        "        \n",
        "        return individual\n",
        "\n",
        "    def addLayer(self, individual):\n",
        "        l = random.randint(0, len(individual.layers)-1)\n",
        "        individual.layers.insert(l, Layer().randomInit())\n",
        "        individual.strategy.blocks.insert(l, Block().randomInit())\n",
        "        return individual \n",
        "\n",
        "    def delLayer(self, individual):\n",
        "        if len(individual.layers)>1:\n",
        "            l = random.randint(0, len(individual.layers)-1)\n",
        "            del individual.layers[l]\n",
        "            del individual.strategy.blocks[l]\n",
        "        return individual, \n",
        "    \n",
        "    def mutate(self, individual): \n",
        "\n",
        "        mutfunc = roulette([self.mutateLayer, self.addLayer, self.delLayer],\n",
        "                           [PROB_MUTATE_LAYER, PROB_ADD_LAYER, PROB_DEL_LAYER])\n",
        "        if mutfunc:\n",
        "            return mutfunc(individual)\n",
        "\n",
        "        assert len(individual.layers) == len(individual.strategy.blocks)\n",
        "        \n",
        "        return individual\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bQDyL8LTBwS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pool, multiprocessing\n",
        "\n",
        "from multiprocessing import Process, Queue\n",
        "\n",
        "def worker(name, evalfunc, querries, answers):\n",
        "    while True:\n",
        "        querry = querries.get()\n",
        "        #print(\"worker \", name, \"evaluating\")\n",
        "        answer = evalfunc(querry)\n",
        "        #print(\"worker \", name, \"finished\")\n",
        "        answers.put(answer)\n",
        "\n",
        "    \n",
        "class Pool:\n",
        "    \"\"\" Pool of workers. Workers consume tasks from the querries queue and \n",
        "        feed answers to the answers queue.\n",
        "    \"\"\"\n",
        "\n",
        "    \n",
        "    def __init__(self, processors, evalfunc):\n",
        "\n",
        "        self.querries = Queue()\n",
        "        self.answers = Queue()\n",
        "\n",
        "        self.workers = []\n",
        "        for i in range(processors):\n",
        "             worker_i = Process(target=worker, args=(i, evalfunc, self.querries, self.answers))\n",
        "             self.workers.append(worker_i)\n",
        "             worker_i.start()        # Launch worker() as a separate python process\n",
        "\n",
        "    def putQuerry(self, querry):\n",
        "        self.querries.put(querry)\n",
        "             \n",
        "    def getAnswer(self):\n",
        "        return self.answers.get()\n",
        "\n",
        "\n",
        "    def close(self):\n",
        "        for w in self.workers:\n",
        "            w.terminate()\n",
        "        #print(\"pool killed\")\n",
        "        \n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TlWgfW_Swwu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Paralg\n",
        "\n",
        "import random\n",
        "import pickle\n",
        "import time\n",
        "import datetime \n",
        "from operator import attrgetter \n",
        "\n",
        "from deap import algorithms \n",
        "\n",
        "PROCESSORS = 10\n",
        "\n",
        "def myAsyncEA(population, start_gen, toolbox, cxpb, mutpb, ngen,\n",
        "              stats, halloffame, logbook, verbose, id=None):\n",
        "\n",
        "    \"\"\" Asynchronous EA \"\"\" \n",
        "\n",
        "    def eval_individual(ind):\n",
        "        ind.fitness.values = toolbox.evaluate(ind)\n",
        "        return ind\n",
        "\n",
        "    def gen_new_offspring():\n",
        "        valid_offspring = True\n",
        "        offspring = None\n",
        "        while valid_offspring:\n",
        "            # select two individuals and clone them \n",
        "            offspring = map(toolbox.clone, toolbox.select(population, 2))\n",
        "            # apply crossover and mutation, take the first offspring\n",
        "            offspring = algorithms.varAnd(offspring, toolbox, cxpb, mutpb)[0]\n",
        "            valid_offspring = offspring.fitness.valid\n",
        "            # if the offspring is valid it is already in the population\n",
        "        return offspring \n",
        "\n",
        "    def log_stats():\n",
        "        record = stats.compile(population)\n",
        "        logbook.record(gen=start_gen + (num_evals // popsize),  **record)\n",
        "        if verbose:\n",
        "            print(logbook.stream)\n",
        "\n",
        "    def save_checkpoint():\n",
        "        cp = dict(population=population, generation=start_gen + (num_evals // popsize), halloffame=halloffame,\n",
        "                  logbook=logbook, rndstate=random.getstate())\n",
        "        if id is None:\n",
        "            cp_name = \"checkpoint_ea.pkl\"\n",
        "        else:\n",
        "            cp_name = \"checkpoint_ea_{}.pkl\".format(id)\n",
        "        pickle.dump(cp, open(cp_name, \"wb\"))\n",
        "            \n",
        "    \n",
        "    popsize = len(population)\n",
        "    MIN_POP_SIZE = popsize // 2 \n",
        "    total_time = datetime.timedelta(seconds=0)\n",
        "\n",
        "    pool = Pool(processors=PROCESSORS, evalfunc=eval_individual)    \n",
        "\n",
        "    db = Database()\n",
        "    \n",
        "    if all([ind.fitness.valid for ind in population]):\n",
        "        print(\"All individuals have valid fitness\")\n",
        "        # all inds are valid (loaded from checkpoint), generate some new to fill the pool \n",
        "        for _ in range(PROCESSORS):\n",
        "            offspring = gen_new_offspring() \n",
        "            pool.putQuerry(offspring)\n",
        "    else:\n",
        "        # put all individuals to the pool \n",
        "        for ind in population:\n",
        "            pool.putQuerry(ind)\n",
        "        population = [] # we get them back later \n",
        "\n",
        "    num_evals = 0\n",
        "\n",
        "    start_time = datetime.datetime.now()\n",
        "    while num_evals < (ngen-start_gen)*popsize:\n",
        "\n",
        "        # get finished individual and add him to population \n",
        "        ind = pool.getAnswer()\n",
        "        assert ind.fitness.valid\n",
        "        # add fitness and individual to the database\n",
        "        db.insert(ind, ind.fitness.values[0])\n",
        "        population.append(ind)\n",
        "        num_evals += 1\n",
        "        \n",
        "        # update halloffame\n",
        "        halloffame.update([ind])\n",
        "\n",
        "        if len(population) < MIN_POP_SIZE:\n",
        "            continue\n",
        "\n",
        "        if num_evals % popsize == 0:\n",
        "            # record statistics and save checkpoint\n",
        "            log_stats()            \n",
        "            save_checkpoint()\n",
        "            \n",
        "        # check time    \n",
        "        eval_time = datetime.datetime.now() - start_time\n",
        "        total_time = total_time + eval_time\n",
        "        #print(\"Time \", total_time)\n",
        "        if total_time > datetime.timedelta(hours=8*24):\n",
        "            print(\"Time limit exceeded.\")\n",
        "            break\n",
        "        start_time = datetime.datetime.now() \n",
        "\n",
        "        \n",
        "        if len(population) > popsize:\n",
        "            # delete the worst one\n",
        "            population = list(sorted(population, key=attrgetter('fitness')))[1:]\n",
        "\n",
        "        # generate new offspring and put him to the pool \n",
        "        offspring = gen_new_offspring() \n",
        "        pool.putQuerry(offspring)\n",
        "\n",
        "\n",
        "    pool.close()\n",
        "    #    print(db.data)\n",
        "    db.save(\"database.\" + id + \".pkl\")\n",
        "    return population, logbook\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZ6t3gu_TH9P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Strategy \n",
        "\n",
        "import random\n",
        "\n",
        "class Block:\n",
        "\n",
        "    \"\"\" Strategy for one layer.\n",
        "    \"\"\" \n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def randomInit(self):\n",
        "        self.size = random.uniform(Config.SIZE_MIN_STRATEGY, Config.SIZE_MAX_STRATEGY)\n",
        "        self.dropout = random.uniform(Config.DROPOUT_MIN_STRATEGY, Config.DROPOUT_MAX_STRATEGY)\n",
        "        return self\n",
        "\n",
        "\n",
        "class Strategy:\n",
        "\n",
        "    def randomInit(self, size):\n",
        "        self.blocks = [ Block().randomInit() for _ in range(size) ]\n",
        "\n",
        "        \n",
        "def initIndStrategy(indclass, strategyclass):\n",
        "    ind = indclass()\n",
        "    ind.randomInit()\n",
        "    ind.strategy = strategyclass()\n",
        "    ind.strategy.randomInit(len(ind.layers))\n",
        "    return ind \n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8muh2vDTcqF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 871
        },
        "outputId": "7198ef54-0ec6-4d80-833d-98f395f6a259"
      },
      "source": [
        "# MAIN\n",
        "\n",
        "import sys\n",
        "import array\n",
        "import random\n",
        "import pickle\n",
        "import numpy as np\n",
        "import multiprocessing \n",
        "\n",
        "\n",
        "from deap import algorithms\n",
        "from deap import base\n",
        "from deap import creator\n",
        "from deap import tools\n",
        "\n",
        "\n",
        "#import argparse\n",
        "\n",
        "#parser = argparse.ArgumentParser()\n",
        "#parser.add_argument('--trainset', help='filename of training set')\n",
        "#parser.add_argument('--testset', help='filename of test set')\n",
        "#parser.add_argument('--id', help='computation id')\n",
        "#parser.add_argument('--checkpoint', help='checkpoint file to load the initial state from')\n",
        "#parser.add_argument('--config', help='json config filename')\n",
        "\n",
        "#args = parser.parse_args()\n",
        "##trainset_name = args.trainset\n",
        "#testset_name = args.testset \n",
        "#id = args.id\n",
        "#if id is None:\n",
        "    ##id = \"\" \n",
        "#checkpoint_file = args.checkpoint\n",
        "#config_name = args.config\n",
        "#if config_name is not None:\n",
        "    #load_config(config_name)\n",
        "\n",
        "# for classification fitness is accuracy, for approximation fitness is error\n",
        "if Config.task_type == \"classification\":\n",
        "    creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
        "else:\n",
        "    creator.create(\"FitnessMax\", base.Fitness, weights=(-1.0,))\n",
        "creator.create(\"Individual\", Individual, fitness=creator.FitnessMax)\n",
        "\n",
        "toolbox = base.Toolbox()\n",
        "\n",
        "# Structure initializers\n",
        "toolbox.register(\"individual\", initIndividual, creator.Individual)\n",
        "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
        "\n",
        "# use multiple processors \n",
        "pool = multiprocessing.Pool(5)\n",
        "toolbox.register(\"map\", pool.map)\n",
        "\n",
        "\n",
        "# load the whole data \n",
        "X_train, y_train, X_test, y_test = load_dataset()\n",
        "\n",
        "# register operators \n",
        "fit = Fitness(X_train, y_train)\n",
        "mut = Mutation()\n",
        "cross = Crossover()\n",
        "\n",
        "toolbox.register(\"evaluate\", fit.evaluate)\n",
        "toolbox.register(\"mate\", cross.cxOnePoint)\n",
        "toolbox.register(\"mutate\", mut.mutate)\n",
        "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
        "\n",
        "def main(id, checkpoint_name=None):\n",
        "    # random.seed(64)\n",
        "\n",
        "    if checkpoint_name:\n",
        "        # A file name has been given, then load the data from the file\n",
        "        cp = pickle.load(open(checkpoint_name, \"rb\"))\n",
        "        pop = cp[\"population\"]\n",
        "        start_gen = cp[\"generation\"] + 1\n",
        "        hof = cp[\"halloffame\"]\n",
        "        logbook = cp[\"logbook\"]\n",
        "        random.setstate(cp[\"rndstate\"])\n",
        "    else:\n",
        "        pop = toolbox.population(n=Config.pop_size)\n",
        "        start_gen = 0\n",
        "        hof = tools.HallOfFame(1)\n",
        "        logbook = tools.Logbook()\n",
        "    \n",
        "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
        "    stats.register(\"avg\", np.mean)\n",
        "    stats.register(\"std\", np.std)\n",
        "    stats.register(\"min\", np.min)\n",
        "    stats.register(\"max\", np.max)\n",
        "    \n",
        "    pop, log = myEASimple(pop, start_gen, toolbox, cxpb=0.6, mutpb=0.2, ngen=Config.ngen, \n",
        "                              stats=stats, halloffame=hof, logbook=logbook, verbose=True,\n",
        "                              id=id)\n",
        "\n",
        "    return pop, log, hof\n",
        "\n",
        "\n",
        "\n",
        "# load the whole data \n",
        "#X_train, y_train, X_test, y_test = load_dataset()\n",
        "\n",
        "    \n",
        "# set cfg\n",
        "Config.input_shape = X_train[0].shape \n",
        "Config.noutputs = y_train.shape[1]\n",
        "    #    print(Config.input_shape, Config.noutputs)\n",
        "\n",
        "\n",
        "\n",
        "#if checkpoint_file is None:\n",
        "pop, log, hof = main(id)\n",
        "#else:\n",
        "    #pop, log, hof = main(id, checkpoint_file)\n",
        "\n",
        "    \n",
        "network = hof[0].createNetwork()\n",
        "network.summary()\n",
        "print( hof[0] )\n",
        "print( hof[0].fitness )\n",
        "\n",
        "\n",
        "    # learn on the whole set\n",
        "    #    \n",
        "E_train, E_test = [], []  \n",
        "\n",
        "for _ in range(10):\n",
        "  network = hof[0].createNetwork() \n",
        "  network.fit(X_train, y_train, batch_size=Config.batch_size, nb_epoch=Config.epochs, verbose=0)\n",
        "\n",
        "  yy_train = network.predict(X_train)\n",
        "  E_train.append(error(yy_train, y_train)) \n",
        "        \n",
        "  yy_test = network.predict(X_test)\n",
        "  E_test.append(error(yy_test, y_test))\n",
        "\n",
        "        \n",
        "def print_stat(E, name):\n",
        "    print(\"E_{:6} avg={:.4f} std={:.4f}  min={:.4f} max={:.4f}\".format(name,\n",
        "                                                                           np.mean(E),\n",
        "                                                                           np.std(E),\n",
        "                                                                           np.min(E),\n",
        "                                                                           np.max(E)))\n",
        "        \n",
        "print_stat(E_train, \"train\")\n",
        "print_stat(E_test, \"test\")\n",
        "\n",
        "    "
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/deap/creator.py:141: RuntimeWarning: A class named 'FitnessMax' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
            "  RuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/deap/creator.py:141: RuntimeWarning: A class named 'Individual' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
            "  RuntimeWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Data loaded\n",
            "Preparing pixels\n",
            "Pixel data ready\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n    result = (True, func(*args, **kwds))\n  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n    return list(map(*args))\n  File \"<ipython-input-11-98c7b78ccd97>\", line 47, in evaluate\n    batch_size=Config.batch_size, epochs=Config.epochs, verbose=0)\n  File \"/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\", line 1154, in fit\n    batch_size=batch_size)\n  File \"/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\", line 621, in _standardize_user_data\n    exception_prefix='target')\n  File \"/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\", line 135, in standardize_input_data\n    'with shape ' + str(data_shape))\nValueError: Error when checking target: expected activation_7 to have 4 dimensions, but got array with shape (48000, 10)\n\"\"\"",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-0981964b08b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;31m#if checkpoint_file is None:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m \u001b[0mpop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhof\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;31m#else:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;31m#pop, log, hof = main(id, checkpoint_file)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-0981964b08b5>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(id, checkpoint_name)\u001b[0m\n\u001b[1;32m     91\u001b[0m     pop, log = myEASimple(pop, start_gen, toolbox, cxpb=0.6, mutpb=0.2, ngen=Config.ngen, \n\u001b[1;32m     92\u001b[0m                               \u001b[0mstats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhalloffame\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhof\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogbook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogbook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m                               id=id)\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhof\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-5d1b23e2c521>\u001b[0m in \u001b[0;36mmyEASimple\u001b[0;34m(population, start_gen, toolbox, cxpb, mutpb, ngen, stats, halloffame, logbook, verbose, id)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# Evaluate the individuals with an invalid fitness\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0minvalid_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpopulation\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfitness\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mfitnesses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoolbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoolbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minvalid_ind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minvalid_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfitnesses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfitness\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         '''\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    642\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected activation_7 to have 4 dimensions, but got array with shape (48000, 10)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHcjJfQOVBUz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}